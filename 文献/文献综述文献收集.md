# 文献综述文献收集

## 一、参考写法

[如何写文献综述](https://codeantenna.com/a/0l6GVYCcK2)



## 二、参考文献

### 2.1《2019/3/17 A Deep Learning-Based Approach for Multi-Label Emotion Classifification in Tweets》

applied sciences

> 目前，人们使用Twitter或Facebook等在线社交媒体来分享他们的情绪和想法。检测和分析社交媒体内容中表达的情感，在商业、公共卫生、社会福利等领域具有重要的应用价值。以往的情感和情绪分析工作大多只关注于单标签分类，忽略了同一实例中存在多个情感标签的情况。该文介绍了一种新颖的基于深度学习的系统，用于解决Twitter中的多情感分类问题。本文提出一种新的方法将其转换为二分类问题，并利用深度学习方法来解决转换后的问题。该系统的表现超过了最先进的系统，在具有挑战性的SemEval2018任务1:e- c多标签情感分类问题上取得了0.59的准确率分数。

有详细介绍**问题转化**的部分，相当于解决多标签分类的办法。该文对BR的方法介绍的很详细还有相应的参考文献，可以拿过来

• 我们针对多标签分类问题，提出了一种新的转换机制。

• 我们提出一种新的基于注意力机制的深度学习系统，称为二值神经网络(BNet)，基于新的转换方法工作。该系统是一个数据驱动的、基于神经的端到端模型，不依赖于词性标记器和情感或情感词典等外部资源。

• 在 SemEval-2018 Task1: Affect in Tweets 的具有挑战性的多标签情绪分类数据集上评估了所提出的系统。

• 实验结果表明，该系统的性能优于目前最先进的系统。



### 2.2 《2022/3/3 Attention-based Region of Interest (ROI) Detection for Speech Emotion Recognition》

arXiv

> 现实生活中的自动情感识别是一项具有挑战性的任务。人类的情感表达是微妙的，可以通过多种情感的组合来表达。在现有的情感识别研究中，大多数音频/视频片段都是完整地进行标注/分类的。然而，话语/片段级的标记和分类可能太粗，无法捕捉细微的话语内/片段时间动态。例如，一个话语/视频片段通常只包含少量的情感显著区域和许多无情感区域。本文提出在深度循环神经网络中使用注意力机制来检测人类情感语音/视频中情感上更显著的感兴趣区域(ROI)，并通过聚合这些情感上显著的感兴趣区域来进一步估计时间情感动态。我们比较了音频和视频的ROI，并对它们进行了分析。将所提出的注意力网络与目前最先进的LSTM模型在识别六种基本人类情感的多分类任务上的性能进行了比较，所提出的注意力模型表现出明显更好的性能。此外，注意力权重分布可以用来解释一个话语如何被表达为可能的情感的混合。

也许谈到情感分类的应用的时候能用上



### 2.3 《2018/4/17 AttnConvnet at SemEval-2018 Task 1: Attention-based Convolutional Neural Networks for Multi-label Emotion Classifification》

arXiv

> 该文提出了一种基于注意力的分类器，可以对给定句子的多种情感进行预测。该模型模拟了人类理解句子的两步过程，能够有效地表示和分类句子。通过表情符号到意思的预处理和额外的词典利用，进一步提高了模型的性能。使用SemEval-2018任务1-5提供的数据训练和评估模型，其中每个句子都有11种给定情感中的几个标签。我们的模型在英语和西班牙语中分别取得了第五和第一名。

注意力机制是深度学习中最受关注的趋势之一，最近也进入了NLP。应用于具有神经网络的系统，它的功能就像人类的视觉注意机制(Denil et al.， 2012)，随着时间的推移，最有效的特征区域将被突出显示，使系统更好地利用

受(Vaswani et al.， 2017)的启发，我们提出了一种基于注意力的多标签句子分类器，可以有效地表示和分类句子。该系统由一个自注意力模块和多个cnn组成，使其能够模仿人类分析句子的两步过程:理解和分类。此外，表情符号到含义的预处理和额外词汇的利用提高了模型在给定数据集上的性能。我们在(Mohammad等人，2018)的数据集上评估了我们的系统，它在英语和西班牙语中分别排名第五和第一。



然后，人类分别将句子分类到每种情感，**而不是一次性全部分类，这就是我们的系统使用11个独立cnn的原因。**除了主体结构之外，由于预处理在NLP任务中占很大比例，特别是当



我们的系统主要处理数量有限的推文数据，这些数据噪声很大。在这种情况下，数据预处理对模型性能有至关重要的影响。表情符号可能是推文的一个典型属性，我们发现相当多的推文包含表情符号。**每个表情符号都有自己的含义，我们将数据中的每个表情符号转换为代表其含义的短语/单词。**我们将这个过程称为表情符号到含义（emoji-to-meaning）的预处理。有些推文过多地重复某些表情符号，可能会使句子过于偏向某些情绪。出乎意料的是，移除重叠的表情降低了性能。



嵌入:在处理小型数据集时，使用预训练的词嵌入特别有用。在那些众所周知的词嵌入中，如Word2Vec(Mikolov等人，2013)，GloVe(Pennington et al.， 2014)和fastText(Piotr et al.， 2016)，我们对英语采用300维GloVe向量，它是在8400亿个令牌的Common Crawl数据上训练的，对西班牙语采用300维fastText向量，它是在维基百科上训练的。



自注意力:**Vaswani et al.(2017)提出了一种基于点积注意力模块的非循环机器翻译架构Transformer**。通常，注意力机制被用作深度学习模型的子模块，在给定序列的情况下计算每个位置的重要性权重。在该系统中，我们采用了(Vaswani et al.， 2017)中的自注意力机制来表示句子。self-attention的详细结构如图2所示。每个嵌入向量与权重矩阵W∈R de×3de的点积通过维度划分为相同大小的Q、K、V，其中de为嵌入向量的维度。然后，出席向量的计算如(3)所示。

并使用每个类的交叉熵之和作为最终的损失函数。

表1显示了同一模型在不同预处理下的准确率。我们发现，表情符号到含义的预处理可以将模型的准确率提高1%。当一个表情符号被转换成它的意思时，它可以被表示为情感词汇的组合，不仅可以减少冗余词汇，还可以进一步强调某些情绪的影响。



该文提出了一种基于注意力机制的句子分类器，可以将一个句子分类为多种情感。实验结果表明，该系统具有有效的句子理解结构。**我们的系统大致遵循人类对句子进行多标签分类的过程。然而，有些情绪可能有一定的相关性，而我们的模型则独立地对待它们。在未来的工作中，我们希望进一步考虑情感之间的潜在联系。**

### 2.4 《2021/6/21 Basic and Depression Specifific Emotion Identifification in Tweets: Multi-label Classifification Experiments》

arXiv

> 该文利用目前最先进的多标签分类器，分别对推文的基础情感挖掘和针对抑郁症的多情感挖掘进行了实证分析。从4种情绪的心理学模型中选取常见情绪组成一个混合情绪模型，从中选择基本情绪。此外，我们用新的情绪类别来增强该情绪模型，因为它们在抑郁症分析中的重要性。这些额外的情感大多没有被用于以往的情感挖掘研究中。实验分析表明，**代价敏感的RankSVM算法和深度学习模型都是鲁棒的，无论是宏观f -度量还是微观f -度量。**这表明，这些算法在解决多标记学习中众所周知的数据不平衡问题方面具有优越性。此外，**深度学习的应用表现最好，在建模扩展情感类别的深度语义特征方面具有优势。**

这表明了我用LSTM比机器学习的理由



由于人类情感倾向于同时出现在[12]中，因此挖掘多种人类情感是一个有趣的研究领域。例如，最常见的人类情感，如喜悦和惊喜，往往是同时发生的，而不仅仅是喜悦或惊喜。(有关我们数据集的一些示例，请参见表1)。此外，识别这些情绪的共现及其构成可以为各种心理健康问题中情绪的细粒度分析提供洞察。很少有文献探索了从文本中挖掘多标签情感[2,11,17]。随着人们越来越多地使用社交媒体，在社交媒体上分享他们的日常想法和想法，在他们的帖子中更容易捕捉到不同的情绪。因此，本研究的主要重点是为从社交媒体帖子(如推文)中识别多情绪提供见解。为了编制我们想要识别的情绪列表，我们使用了一个混合情绪模型[17]，该模型基于心理学中使用的四种不同且广泛使用的情绪模型。此外，我们用更多的情绪来增强这个情绪模型，这些情绪被认为对我们打算稍后进行的抑郁症识别任务有用。在这里，我们将实验分为两个:一个用于较小的情感模型(使用9种基本的人类情感)，另一个用于增强的情感模型(同时使用基本的和与抑郁相关的人类情感)。针对这两类数据都具有不同程度的数据不平衡性，详细分析了用于多标签文本挖掘任务的几种最新算法在这两类数据上的性能。



在情感计算研究中，被广泛接受的情感模型是[5]提出的情感模型。该模型由六种情绪组成:愤怒、厌恶、恐惧、喜悦、悲伤和惊讶。本文采用了最近在[17]中提出的情感模型，该模型是这些模型的推广，并添加了少量与我们的研究相关的额外情感:爱、感恩和内疚。我们进一步寻求确认诸如背叛、沮丧、绝望、孤独、拒绝、幸灾乐祸和自我厌恶等情绪;任何这些都可能有助于识别抑郁症[1,16]。**在RankSVM和基于注意力的深度学习模型的帮助下**，我们对这些情感的挖掘是以前缺乏的新贡献[13,7]。



PTMs的替代类别是所谓的算法自适应方法(AAMs)，其中将单标记分类器修改为进行多标记分类。目前流行的AAMs是基于树的，如经典的C4.5算法多标签任务[4]，概率模型如[6]，以及基于神经网络的方法如BP-MLL[19]。然而，与PTMs一样，**这些AAM方法也不是为不平衡数据量身定制的，在常见的不平衡多标签数据集上无法取得很好的准确性。在我们的方法中，我们探索了两种最先进的多标签分类方法。一个是代价敏感的RankSVM，另一个是基于长短期记忆网络(Long Short Term Memory, LSTM)和注意力的深度学习模型。**前者是PTM和AAMs的融合，并具有大间隔分类器的额外优势。这为基于大规模不平衡多标记数据的学习提供了优势，同时仍考虑了标记之间的相关性。后者是一种纯粹的AAM方法，能够更准确地捕捉推文的潜在语义结构。在第2节和第3节中，我们提供了基线模型和实验模型的技术细节。



**该文专门弄了个Baseline Model专栏挺好的可以借鉴**



**对LSTM进行了概述，可以借鉴**，不过人家的直接是多标签输出

[20]的实验结果表明，长短期记忆网络(LSTM)和LSTM的一种替代方法——门控循环单元层(GRU)的组合在学习短语级特征时非常有用，在文本分类中取得了非常好的精度。在引入自注意力机制的双向LSTM (bi-LSTM)的帮助下，[9]在句子分类方面取得了目前最先进的效果。本文采用[9]模型，并通过使用适当的损失函数和阈值softmax层来生成多标签输出，进一步使该模型用于多标签分类。我们将此模型称为lstmatattention (LSTM-att)，如图1所示;wi是词嵌入(可以是单热词袋或密集词向量)，hi是LSTM在时间步i的隐藏状态，该层的输出被馈送到Self Attention (SA)层。SA层的输出然后发送到线性层，在softmax激活的帮助下，将最终输出转换为不同标签的概率(在这种情况下是情绪)。最后，对softmax输出施加阈值以获得最终的多标签预测。



**有loss function介绍的模块而且是binary crossentropy**



![](LSTM+self-attention结构.jpg)



**该文的预处理和评价指标的介绍可以参考**



深度学习模型使用嵌入向量的优势

As expected,this confifirms that Deep Learning models are good with dense word vectors rather thanvery sparse bag-of-words features.



### 2.5 《2022/6/12 DeepEmotex: Classifying Emotion in Text Messages using Deep Transfer Learning》

arXiv

> **迁移学习**通过深度预训练语言模型在自然语言处理中得到广泛应用，如transformer的双向编码器表示和通用句子编码器。**尽管取得了巨大的成功，但语言模型在应用于小数据集时容易过拟合，并且在用分类器进行微调时容易遗忘。**为解决深度预训练语言模型从**一个域迁移到另一个域时的遗忘问题**，现有工作探索了微调方法以减少遗忘。本文提出DeepEmotex，一种有效的**序列迁移学习方法**来检测文本中的情感。为了避免遗忘问题，微调步骤是通过从Twitter上收集的大量情感标记数据进行的。
>
> 使用策划的Twitter数据集和基准数据集进行了一项实验研究。DeepEmotex模型在测试集上的多类情感分类准确率超过91%。在EmoInt和Stimulus基准数据集上评估了微调后的DeepEmotex模型在情感分类方面的性能。在基准数据集上，模型有73%的实例正确分类了情感。所提出的DeepEmotex-BERT模型在基准数据集上比Bi-LSTM结果高出23%。还研究了微调数据集的大小对模型精度的影响。评估结果表明，用大量情感标记数据进行微调，提高了所产生的目标任务模型的鲁棒性和有效性。

可以在介绍完有transfomer后说什么什么人在在transfromer的基础上进行的改进



由于文本中情感表达的语义模糊以及情感类别[1]的边界模糊，文本中的情感检测是一个具有挑战性的问题。此外，与单独评估句子时的感知情绪相比，上下文可以完全改变句子的情绪。例如，这句话“I started crying when I realize !”“会被认为是一种悲伤的感觉，然而考虑到它的背景下，“我刚刚合格的奖学金。当我意识到的时候，我开始哭了!，结果是一种快乐的情绪。



监督学习方法在情感分类中被证明是有前途的。然而，这些方法是域依赖的，这意味着建立在一个域上的模型(例如，关于特定主题或事件的消息)可能在另一个域上表现不佳。原因是不同领域的词和短语可能会被用来表达不同领域的情感。表一显示了两个不同领域的常用关键词，George Floyd之死和Covid-19大流行。例如，“正义”、“抗议”、“暴力”、“谋杀”、“种族主义”等是第一个领域特有的领域关键词，而“死亡”、“疾病”、“发烧”等是新冠病毒特有的领域关键词。**由于不同领域之间的公共关键词不匹配，在一个领域上训练的情感分类器直接应用到另一个领域时可能效果不佳。**因此，跨领域情感分类方法是可取的。这种方法通过将知识从相关领域转移到感兴趣的领域[3]，减少了领域依赖性，以及手动标记训练监督学习模型所需的足够数量的示例文本所需的成本和人力。



不同的神经网络架构已经出现来应对NLP任务中的挑战，如rnn, LSTM, BiLSTM。最近，**transformer架构[7]已被证明在NLP任务中具有非常有希望的结果。**transformer语言模型表现出了更好的语言理解能力，使它们在许多不同的NLP任务中取得了最先进的结果。



虽然深度学习模型在文本分类任务上取得了最先进的结果，但这些模型是从头开始训练的，需要大量的输入数据集，需要几天的时间来收敛[8]。迁移学习可以用来将知识从通用领域和任务迁移到更专业的目标领域和任务[9]，[10]，[11]，而不是从头开始学习。事实上，迁移学习在许多情况下可以达到甚至超过为特定任务从头开始训练的传统深度学习模型的性能，但只需要更少的标记样本集来对目标任务[8]进行微调。迁移学习也可以用于文本中的情感和情感分类。



**迁移学习通过预训练语言模型(LMs)[12]在自然语言处理(NLP)中得到了广泛应用。深度预训练语言模型，如通用句子编码器(USE)[13]和transformer双向编码器表示(BERT)[14]，在自然语言处理(NLP)中得到广泛应用。**尽管取得了巨大的成功，但语言模型在小数据集上会过拟合，并且在用分类器[8]进行微调时容易发生灾难性的遗忘。为了补救深度预训练语言模型迁移中的灾难性遗忘，现有工作主要探索微调技巧以减少遗忘。微调实验依赖于预训练语言模型参数。在目标任务训练期间，语言模型必须有足够的适应性，以能够解决与预训练模型不同的输入分布和输出标签空间的目标任务。自适应步骤还必须避免过拟合或忘记在预训练[15]期间学到的东西。



•本文开发了DeepEmotex，用深度迁移学习对短信中的情感进行分类。DeepEmotex开发了迁移学习模型，用于微调预训练语言模型，并学习更能感知新领域上下文的情感特定特征。

•**DeepEmotex利用两个最先进的预训练模型，称为BERT[14]和USE[13]。为了避免对目标任务过度拟合，我们利用了从Twitter消息中收集的大量情绪标记数据。**

•分析预训练知识迁移到情绪分类任务的自适应或微调阶段。通过微调BERT, DeepEmotex在我们的测试数据上取得了92%的分类准确率。

•评估了DeepEmotex模型在EmoInt和Stimulus基准数据集上对情绪进行分类的性能。在基准数据集中，DeepEmotex模型能够正确地对73%的实例进行情感分类。

•验证了微调任务的数据规模对模型准确性的影响。评估结果表明，对一个大的情感标记数据集(超过30K)进行微调，提高了所产生的目标任务模型的鲁棒性和有效性。

•通过将DeepEmotex模型的结果与基线模型(即Bi-LSTM)进行比较来评估它们。实验结果表明，DeepEmotex-BERT模型比基线模型性能提高了23%。



**该文用了很大的篇幅的一部分介绍了深度学习在情感分类上的工作和发展可以借鉴**



Kratzwald等人提出的Sent2affect是一种基于迁移学习的情感计算方法。他们的BiLSTM网络是针对不同的任务(即情感分析)进行预训练的，而输出层随后被调整为情感识别任务。在6个基准数据集上对所产生的性能进行了评估，他们发现在所有实验中，使用预训练词嵌入的BiLSTM方法是更好的方法。在所有实验中，BiLSTM似乎都优于单向LSTM。Batbaatar等人[26]提出了一种结合两个子网络来捕捉语义和情感信息的语义-情感神经网络(semantic-emotion neural network, SENN)。第一个网络由BiLSTM组成，用于捕获语义信息。第二个网络是用于捕捉情感信息的CNN。在ISEAR数据集上，SENN模型的性能约为%74。Imran等[27]分析了不同文化背景的公民对新冠病毒的情感。用于从提取的推文中估计情感极性和情感的LSTM模型已经经过训练，在sentiment140数据集上达到了最先进的精度。



最近，上下文化的词嵌入被提出，称为USE[13]和BERT[14]，以在传统的词嵌入中纳入上下文信息。然而，这些词嵌入在各种任务上都是泛化的，并且局限于提供情绪信息，**因此使用神经网络学习特定任务的情绪嵌入被证明是有效的。一些研究人员预测文本对话中的情感**。Luo和Wang[29]通过从四种情绪类别中选择高兴、悲伤、愤怒和中性，**对BERT模型进行微调**，以预测对话中的情绪。他们使用的数据集由电视节目、朋友和名为EmotionPush的匿名Facebook聊天日志组成。Chatterjee et al.[30]使用BiLSTM模型从文本对话中选择四种情绪类别(高兴、悲伤、愤怒和其他)来推断潜在的情绪。EmoDet2是al - omari等人开发的另一种对对话中的情绪进行分类的工作。他们将EmoContext数据集分为快乐、悲伤、愤怒和其他。他们使用GloVe嵌入和从情感推文中提取的特征。他们还从BERT模型中提取单词上下文嵌入。这些向量馈入前馈和BiLSTM模型以获得预测。他们的结果表明，通过提取BERT嵌入并将其输入到BiLSTM网络，系统的性能得到了提高。

**迁移学习旨在利用来自源任务的知识来提高模型在不同但相关的目标任务中的性能。序列迁移学习依次学习源任务和目标任务，从源任务中迁移知识以提高模型在目标任务上的性能[9]，[11]。顺序迁移学习通常包括两个阶段:预训练和自适应。在预训练期间，模型在源任务上进行训练。在自适应过程中，对预训练模型进行目标任务训练。标准的自适应方法包括微调和特征提取。微调更新预训练模型的参数，而特征提取则将预训练模型作为特征提取器，并在自适应阶段[20]，[32]保持参数固定。近年来，序列迁移学习得到了广泛应用，深度预训练语言模型在各种NLP任务[14]、[33]上取得了巨大成功。虽然深度预训练模型的自适应非常高效，但容易出现遗忘，即在学习目标任务[8]时，模型会忘记以前从源任务中学习到的知识**

深度预训练语言模型，如USE[13]和BERT[14]，已被用于自然语言处理。这些预训练语言模型通过顺序迁移学习促进了广泛的NLP任务[8]:在大规模未标记数据上预训练语言模型，然后使其适应下游任务。自适应步骤通常有两种方法进行:微调或冻结预训练权重。在实际应用中，微调因其灵活性而被更广泛地采用。DeepEmotex采用顺序迁移学习方法，源和目标任务是按顺序学习的。这意味着，模型不像在多任务学习中那样联合优化，而是每个任务分别学习[11]。序列迁移学习由两个阶段组成:**预训练阶段，在源任务或域上学习通用表示;自适应或微调阶段，将学习到的知识迁移到目标任务或域[11]。**对于情感分类来说，使用深度神经网络架构并非易事。问题是预训练模型不适合小的情感数据集。当用分类器[8]进行微调时，它们通常会出现过拟合和遗忘问题。当微调数据很小时，这尤其是个问题。为解决深度预训练模型迁移中的遗忘问题，现有工作主要探索微调技巧以减少遗忘。为了进行微调，减少遗忘，我们利用了在Twitter中很容易获得的大量情感标签数据。之前的大多数工作都专注于不同的预训练目标，以学习通用单词或句子表示[5]，[36]。一些作品探索了微调阶段

**有两种常见的微调方法**:第一种方法是使用预训练网络作为特征提取器[38]，在对目标任务进行微调时，模型中的所有层都是冻结的，除了最后一层。在这种方法中，预训练表示被用于下游模型。或者，在新任务[39]上解冻并微调预训练模型的参数。这种方法能够使通用表示适应许多不同的任务。图1显示了我们的DeepEmotex模型。我们的模型通过它们的嵌入来表示输入的单词。在嵌入层之后，我们的模型由transformer编码器组成，然后是SoftMax分类层。更好地了解自适应阶段是充分利用预训练表示的关键。因此，DeepEmotex利用了两个最先进的预训练模型，即BERT[14]和通用句子编码器[13]。利用这些模型，我们将从大型语料库中学习到的知识迁移到我们的情感分类模型中。然后，对DeepEmotex模型进行微调，以适应目标情感数据集。

**预训练模型（如包含transfomer的encoder模型）相当于编码成思想向量后传给下游+ 自适应或微调阶段（用于多标签分类的全连接网络）**

*•* Batch size: 32, 64

*•* Learning rate: 5e-5, 3e-5, 2e-5

*•* Number of epochs: 2, 3, 4



### 2.6 《2018/2/3 Joint Binary Neural Network for Multi-label Learning with Applications to Emotion Classifification》

arXiv

> d